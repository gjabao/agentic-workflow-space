# Job Scraper v2.0 Upgrade - Email-First Decision-Maker Discovery

## Summary

Upgrading 3 job scrapers to use Crunchbase v4.0 email-first workflow for **200-300% decision-maker coverage** (2-3 DMs per company vs 0.3-0.4 before).

---

## Changes Required Per Scraper

### Files to Modify
1. `execution/scrape_indeed_jobs.py`
2. `execution/scrape_linkedin_jobs.py`
3. `execution/scrape_glassdoor_jobs.py`

### Changes Apply to ALL 3 Files

#### Change 1: Add `Tuple` to imports (top of file)
```python
from typing import Dict, List, Optional, Any, Generator, Tuple  # Add Tuple
```

#### Change 2: Add `AnyMailFinderCompanyAPI` class (after imports, before main scraper class)
```python
class AnyMailFinderCompanyAPI:
    """Find ALL emails at company (up to 20) for email-first workflow"""

    BASE_URL = "https://api.anymailfinder.com/v5.1/find-email/company"

    def __init__(self, api_key: str):
        self.api_key = api_key
        logger.info("‚úì AnyMailFinder Company API initialized")

    def find_company_emails(self, company_domain: str, company_name: str = None) -> Dict:
        """Find ALL emails at company in one API call"""
        try:
            headers = {
                'Authorization': self.api_key,
                'Content-Type': 'application/json'
            }
            payload = {
                'domain': company_domain,
                'email_type': 'any'
            }
            if company_name:
                payload['company_name'] = company_name

            response = requests.post(self.BASE_URL, headers=headers, json=payload, timeout=15)

            if response.status_code == 200:
                data = response.json()
                email_status = data.get('email_status', 'not_found')
                if email_status == 'valid' and data.get('valid_emails'):
                    return {'emails': data['valid_emails'], 'status': 'found', 'count': len(data['valid_emails'])}
                return {'emails': [], 'status': 'not-found', 'count': 0}
            return {'emails': [], 'status': 'not-found', 'count': 0}
        except Exception:
            return {'emails': [], 'status': 'not-found', 'count': 0}
```

#### Change 3: Update `__init__` to initialize Company API
Replace the AnyMailFinder Person API initialization with:
```python
# Initialize AnyMailFinder Company API (email-first workflow)
self.anymail_company_api = AnyMailFinderCompanyAPI(self.anymail_key) if self.anymail_key else None
```

#### Change 4: Add helper methods to scraper class
Add these 2 methods to the main scraper class (IndeedJobScraper, LinkedInJobScraper, GlassdoorJobScraper):

```python
def extract_contact_from_email(self, email: str) -> Tuple[str, bool, float]:
    """Extract name from email (firstname.lastname@ ‚Üí "Firstname Lastname")"""
    if not email or '@' not in email:
        return ('', True, 0.0)

    local_part = email.split('@')[0].lower()

    # Skip generic emails
    generic_patterns = ['info', 'contact', 'hello', 'support', 'sales', 'admin', 'office', 'help', 'team', 'hr', 'jobs']
    if any(pattern in local_part for pattern in generic_patterns):
        return ('', True, 0.0)

    # firstname.lastname@
    if '.' in local_part:
        parts = local_part.split('.')
        valid_parts = [p for p in parts if p.isalpha() and 2 <= len(p) <= 20]
        if len(valid_parts) == 2:
            return (f"{valid_parts[0].capitalize()} {valid_parts[1].capitalize()}", False, 0.95)
        elif len(valid_parts) > 2:
            return (f"{valid_parts[0].capitalize()} {valid_parts[-1].capitalize()}", False, 0.9)

    # firstname_lastname@ or firstname-lastname@
    name_parts = re.split(r'[._\-0-9]+', local_part)
    name_parts = [p for p in name_parts if p.isalpha() and 2 <= len(p) <= 20]
    if len(name_parts) >= 2:
        first, last = name_parts[0].capitalize(), name_parts[-1].capitalize()
        return (f"{first} {last}", False, 0.9 if len(first) >= 3 and len(last) >= 3 else 0.7)
    elif len(name_parts) == 1 and len(name_parts[0]) >= 3:
        return (name_parts[0].capitalize(), False, 0.6)

    return ('', False, 0.2)

def is_decision_maker(self, job_title: str) -> bool:
    """Validate if job title is decision-maker"""
    if not job_title:
        return False
    jt_lower = job_title.lower()

    # Must have decision-maker keyword
    dm_keywords = ['founder', 'co-founder', 'ceo', 'chief', 'owner', 'president', 'managing partner',
                   'vice president', 'vp ', 'cfo', 'cto', 'coo', 'cmo', 'executive', 'c-suite', 'partner']
    has_dm = any(kw in jt_lower for kw in dm_keywords)

    # Must NOT have exclude keyword
    exclude_keywords = ['assistant', 'associate', 'junior', 'intern', 'coordinator', 'analyst', 'specialist']
    has_exclude = any(kw in jt_lower for kw in exclude_keywords)

    return has_dm and not has_exclude
```

#### Change 5: Replace `process_single_company()` method
This is the CORE change - replace the entire method with email-first workflow:

```python
def process_single_company(self, job: Dict) -> List[Dict]:
    """
    Process company with EMAIL-FIRST workflow (v2.0):
    1. Find website ‚Üí 2. Find ALL emails ‚Üí 3. Extract names ‚Üí 4. Search LinkedIn ‚Üí 5. Validate DM

    Returns: List of decision-makers (2-5 per company vs 0-1 before)
    """
    company = job['company_name']
    company_type = self.detect_company_type(company, job.get('industry', ''), job.get('job_description', ''))

    # Step 1: Find company website
    website_data = self.find_company_website(company)
    website = website_data.get('url', '')
    domain = self.extract_domain(website) if website else ""

    if not domain:
        logger.info(f"  ‚äò No website for {company} - skipping")
        return []

    # Step 2: Find ALL emails at company (up to 20)
    logger.info(f"  üìß Finding emails at {domain}...")
    if not self.anymail_company_api:
        logger.info(f"  ‚äò AnyMailFinder not configured - skipping")
        return []

    email_result = self.anymail_company_api.find_company_emails(domain, company)
    emails = email_result.get('emails', [])
    logger.info(f"  ‚úì Found {len(emails)} emails")

    if not emails:
        return []

    # Step 3-5: Process each email in PARALLEL (5 workers)
    decision_makers = []
    seen_names_lock = Lock()
    seen_names = set()

    def process_single_email(email: str) -> Optional[Dict]:
        """Extract name ‚Üí Search LinkedIn ‚Üí Validate DM"""
        # Extract name from email
        extracted_name, is_generic, confidence = self.extract_contact_from_email(email)
        if is_generic or not extracted_name or confidence < 0.5:
            return None

        # Search LinkedIn by name + company
        dm = self.find_decision_maker(company)  # Use existing method
        if not dm.get('full_name'):
            return None

        full_name = dm.get('full_name', extracted_name)
        job_title = dm.get('title', '')

        # Thread-safe duplicate check
        with seen_names_lock:
            if full_name in seen_names:
                return None
            seen_names.add(full_name)

        # Validate decision-maker
        if not self.is_decision_maker(job_title):
            return None

        # Build result
        return {
            'company_name': company,
            'company_type': company_type,
            'company_website': website,
            'company_domain': domain,
            'job_title': job.get('job_title', ''),
            'job_url': job.get('job_url', ''),
            'location': job.get('location', ''),
            'dm_name': full_name,
            'dm_first': full_name.split()[0] if full_name else '',
            'dm_last': ' '.join(full_name.split()[1:]) if len(full_name.split()) > 1 else '',
            'dm_title': job_title,
            'dm_linkedin': dm.get('linkedin_url', ''),
            'dm_email': email,
            'email_status': 'found',
            'dm_description': dm.get('description', ''),
            'message': ''
        }

    # PARALLEL processing: 5 workers
    from concurrent.futures import ThreadPoolExecutor, as_completed
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = {executor.submit(process_single_email, email): email for email in emails[:20]}

        for future in as_completed(futures):
            try:
                result = future.result()
                if result:
                    decision_makers.append(result)
                    logger.info(f"  ‚òÖ Found DM: {result['dm_name']} ({result['dm_title']})")
            except Exception as e:
                logger.error(f"  ‚ùå Error processing email: {e}")

    logger.info(f"  ‚úì Found {len(decision_makers)} decision-makers for {company}")
    return decision_makers
```

#### Change 6: Update `execute()` method
Replace the line that processes single companies to handle lists:

**OLD:**
```python
result = future.result()
processed_jobs.append(result)
```

**NEW:**
```python
results = future.result()  # Now returns LIST of DMs
if results:
    processed_jobs.extend(results)  # Extend, not append
```

#### Change 7: Update Google Sheets export headers (if needed)
The headers should already include DM fields, but verify they match:
```python
headers = [
    "Company Name", "Company Type", "Company Website", "Job Title", "Job URL", "Location",
    "DM Name", "DM Title", "DM First", "DM Last", "DM LinkedIn",
    "DM Email", "Email Status", "DM Source", "Message", "Scraped Date"
]
```

---

## Expected Results

### Before (v1.0):
- 20 jobs scraped
- 6-10 decision-makers found (30-40% success rate)
- 1 DM per company (CEO/Founder only)
- Processing time: ~30s per company

### After (v2.0):
- 20 jobs scraped
- **40-60 decision-makers found (200-300% coverage)**
- **2-3 DMs per company (CEO, CFO, CTO, VPs)**
- Processing time: ~10s per company (parallel processing)
- **100% valid emails** (email-first guarantee)

---

## Testing Commands

```bash
# Test Indeed
python3 execution/scrape_indeed_jobs.py --query "AI Engineer" --location "San Francisco" --limit 20

# Test LinkedIn
python3 execution/scrape_linkedin_jobs.py --query "Product Manager" --location "Remote" --limit 20

# Test Glassdoor
python3 execution/scrape_glassdoor_jobs.py --query "CFO" --location "Toronto" --country "Canada" --limit 20
```

---

## Implementation Notes

### Why This Works

1. **Email-First = Guaranteed Quality**
   - Email exists ‚Üí Person exists ‚Üí High confidence

2. **Bulk API = 20x More Data**
   - 1 Company API call ‚Üí 20 emails
   - vs 20 Person API calls (slow + often fails)

3. **Parallel Processing = 3x Faster**
   - 5 workers processing emails simultaneously
   - 20 emails / 5 workers = 4 batches = ~24s
   - vs sequential = 20 emails √ó 6s = 120s

4. **LinkedIn Validation = Quality Filter**
   - Extract name from email ‚Üí Search LinkedIn ‚Üí Validate title
   - Ensures only real decision-makers (not assistants, analysts, etc.)

---

## Rollout Strategy

1. ‚úÖ Read all 3 scrapers
2. ‚úÖ Design upgrade architecture
3. ‚è≥ Apply changes to Indeed
4. ‚è≥ Apply changes to LinkedIn
5. ‚è≥ Apply changes to Glassdoor
6. ‚è≥ Update directives
7. üî≤ User tests all 3 scrapers
8. üî≤ Document results in CLAUDE.md

---

**Status:** Ready for implementation
**User:** Will test after all changes complete
